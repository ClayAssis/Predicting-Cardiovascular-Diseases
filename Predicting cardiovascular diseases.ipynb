{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "# CARDIO CATCH DISEASES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem\n",
    "\n",
    "**Cadio Catch Diseases** is a company that specializes in detecting heart disease in the early stages.The company offers an early diagnosis of cardiovascular disease for a certain price. Currently, the diagosis is made manually. The currently accuracy used to be between 55% and 60%, because of the complex diagnostics and the team's tiredness, there's a another employees on other shifts.\n",
    "\n",
    "There are some points that is important to know:\n",
    "- **Each diagnosis costs about $1000,00**\n",
    "\n",
    "- **The price will vary according to the precision, the customer pays $500.00 for every 5% accuracy above 50%.**\n",
    "\n",
    "- **If the diagnostic precision is 50% or below, the customer doesn't pay for it.**\n",
    "\n",
    "\n",
    "- What is the objective?\n",
    "    - Build a predictive model to classify cardiovascular diseases.\n",
    "    - Why is this necessary?\n",
    "        - Because the price will vary according the precision. \n",
    "\n",
    "- What do we need to show?\n",
    "    - The accuracy and precision of the model.\n",
    "    - How much profit will Cardio Catch Diseases have after the project? \n",
    "    - How reliable is our result?\n",
    "    \n",
    "   \n",
    "- Business problem reference: https://sejaumdatascientist.com/projeto-de-data-science-diagnostico-precoce-de-doencas-cardiovasculares/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cardiovascular Diseases: What is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "Cardiovascular diseases (CVDs) are a group of disorders of the heart and blood vessels and they include:\n",
    "\n",
    "- coronary heart disease – disease of the blood vessels supplying the heart muscle;\n",
    "- cerebrovascular disease – disease of the blood vessels supplying the brain;\n",
    "- peripheral arterial disease – disease of blood vessels supplying the arms and legs;\n",
    "- rheumatic heart disease – damage to the heart muscle and heart valves from rheumatic fever, caused by streptococcal bacteria;\n",
    "- congenital heart disease – malformations of heart structure existing at birth;\n",
    "- deep vein thrombosis and pulmonary embolism – blood clots in the leg veins, which can dislodge and move to the heart and lungs.\n",
    "\n",
    "\n",
    "References: [World Health Organization](\"https://www.who.int/en/news-room/fact-sheets/detail/cardiovascular-diseases-(cvds)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0. IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:29.584663Z",
     "start_time": "2020-10-15T00:21:29.488238Z"
    }
   },
   "outputs": [],
   "source": [
    "#data manipulation\n",
    "import pandas                as pd\n",
    "import numpy                 as np\n",
    "\n",
    "#data visualization\n",
    "import seaborn               as sns\n",
    "import matplotlib.pyplot     as plt\n",
    "from scikitplot              import metrics      as mt\n",
    "\n",
    "#scikit-learn\n",
    "from sklearn.preprocessing   import RobustScaler,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score,cross_val_predict,RandomizedSearchCV\n",
    "from sklearn.ensemble        import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model    import LogisticRegression\n",
    "from sklearn.metrics         import accuracy_score, classification_report,cohen_kappa_score,recall_score,f1_score,roc_auc_score, plot_precision_recall_curve, precision_score,roc_curve\n",
    "from sklearn.pipeline        import Pipeline\n",
    "from sklearn.neighbors       import KNeighborsClassifier\n",
    "from sklearn                 import svm\n",
    "\n",
    "#balanced imports\n",
    "from imblearn import under_sampling as us\n",
    "from imblearn import over_sampling as oversamp\n",
    "from imblearn import combine as c\n",
    "\n",
    "#auxiliar packages\n",
    "from IPython.display         import Image\n",
    "from IPython.core.display    import HTML\n",
    "from scipy                   import stats\n",
    "from math                    import sqrt\n",
    "from tabulate                import tabulate\n",
    "from boruta                  import BorutaPy\n",
    "import xgboost               as xgb\n",
    "from lightgbm                import LGBMClassifier\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:29.673140Z",
     "start_time": "2020-10-15T00:21:29.588477Z"
    }
   },
   "outputs": [],
   "source": [
    "def jupyter_settings():\n",
    "    %matplotlib inline\n",
    "    \n",
    "    plt.style.use( 'bmh' )\n",
    "    plt.rcParams['figure.figsize'] = [25, 12]\n",
    "    plt.rcParams['font.size'] = 24\n",
    "    \n",
    "    display( HTML( '<style>.container { width:100% !important; }</style>') )\n",
    "    pd.options.display.max_columns = None\n",
    "    pd.options.display.max_rows = None\n",
    "    pd.set_option( 'display.expand_frame_repr', False )\n",
    "    \n",
    "    sns.set()\n",
    "    \n",
    "# point biserial correlation coefficient heatmap function\n",
    "def point_bi(a, b):\n",
    "    # a: input dataframe with binary variable\n",
    "    # b: input dataframe with continuous variable\n",
    "    \n",
    "    # get column name\n",
    "    a_name = a.columns[0]\n",
    "    b_name = b.columns[0]\n",
    "    \n",
    "    # transform dataframe to array\n",
    "    a = a.values.reshape(-1)\n",
    "    b = b.values.reshape(-1)\n",
    "    \n",
    "    # apply scipy's point-biserial\n",
    "    stats.pointbiserialr(a, b)\n",
    "    \n",
    "    # correlation coefficient array\n",
    "    c = np.corrcoef(a,b)\n",
    "    \n",
    "    # dataframe for heatmap\n",
    "    df = pd.DataFrame(c, columns=[a_name, b_name], index=[a_name, b_name])\n",
    "    \n",
    "    # return heatmap\n",
    "    return sns.heatmap(df, annot=True).set_title('{} x {} correlation heatmap'.format(a_name, b_name),fontsize = 25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:29.910337Z",
     "start_time": "2020-10-15T00:21:29.678064Z"
    }
   },
   "outputs": [],
   "source": [
    "#setting the function\n",
    "jupyter_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2. Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 70 000 records of patients data in 12 features and the target feature that will tell to us the diagnostic.  We can find the data on  [kaggle](\"https://www.kaggle.com/sulianova/cardiovascular-disease-dataset\") website.\n",
    "\n",
    "- Age | Objective Feature | age | int (days)\n",
    "\n",
    "- Height | Objective Feature | height | int (cm) |\n",
    "- Weight | Objective Feature | weight | float (kg) |\n",
    "- Gender | Objective Feature | gender | categorical code |\n",
    "- Systolic blood pressure | Examination Feature | ap_hi | int |\n",
    "- Diastolic blood pressure | Examination Feature | ap_lo | int |\n",
    "- Cholesterol | Examination Feature | cholesterol | 1: normal, 2: above normal, 3: well above normal |\n",
    "- Glucose | Examination Feature | gluc | 1: normal, 2: above normal, 3: well above normal |\n",
    "- Smoking | Subjective Feature | smoke | binary |\n",
    "- Alcohol intake | Subjective Feature | alco | binary |\n",
    "- Physical activity | Subjective Feature | active | binary |\n",
    "- Presence or absence of cardiovascular disease | Target Variable | cardio | binary |\n",
    "\n",
    "**Let's load the data to start our solution!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:30.237106Z",
     "start_time": "2020-10-15T00:21:29.915847Z"
    }
   },
   "outputs": [],
   "source": [
    "#loading dataset as a dataframe\n",
    "df_raw = pd.read_csv(\"data/cardio_train.csv\", sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0. DATA DESCRIPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, descriptive analysis is the first manipulation performed in a quantitative study and its main objective is to summarize and explore the behavior of the data. This can be done through frequency tables, graphs and numerical summary measures. Before presenting each of these items, let's first make a copy of our dataset. After that, we'll do the follow steps:\n",
    "\n",
    "- Data Dimension\n",
    "- Data Types\n",
    "- Check Missings Values\n",
    "- Change Types\n",
    "- Descriptive Statistical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:30.248248Z",
     "start_time": "2020-10-15T00:21:30.242402Z"
    }
   },
   "outputs": [],
   "source": [
    "#making a copy of the data\n",
    "df1 = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Data Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding how much big is our dataset is, it's essential to comprehend if our infrastructure is enough to run the model or not. It's important to know that there are two data classifications : big data and tradicional data. As we can see below, we have 70000 rows and 13 columns, therefore, we won't need to have a powerful computer to execute the machine learning model. One point that needs to be checked is if we have enough data to ensure learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:30.330142Z",
     "start_time": "2020-10-15T00:21:30.251740Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of rows: {}\".format(df1.shape[0]))\n",
    "print(\"Number of columns: {}\".format(df1.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see bellow, we have 2 data types: **int64** and **float64**. It's important to explain that machine learning algorithms usually build a better learning with numerical data, this is one of the premises that we will assume. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:30.416256Z",
     "start_time": "2020-10-15T00:21:30.333677Z"
    }
   },
   "outputs": [],
   "source": [
    "#checking the types\n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Check Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values are the Achilles' heel of a data scientist. If not handled properly, the entire analysis will be futile and will provide misleading results that can harm business stakeholders. However, as we can analyze below, we did not find any column that has missing data, this will avoid a lot of work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:30.515550Z",
     "start_time": "2020-10-15T00:21:30.427547Z"
    }
   },
   "outputs": [],
   "source": [
    "#checking missing data\n",
    "df1.isnull().sum()/df1.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Change Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"id\" column is a \"int64\" but, we won't execute any mathmatical operation because it is just a indentifying column. Therefore, we could change it to a categorical data or maybe to drop it, but let's to change the type to \"object\". Another point that we need to note is that our \"age\" column has an \"days\" format, so let's switch to the \"age\" format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:30.655353Z",
     "start_time": "2020-10-15T00:21:30.522319Z"
    }
   },
   "outputs": [],
   "source": [
    "#coverting the age column to the years format\n",
    "df1['age_year'] = df1['age'].apply(lambda x: x/365)\n",
    "\n",
    "#age_year convert\n",
    "df1['age_year'] = df1['age_year'].astype(int)\n",
    "\n",
    "#changing id type to object\n",
    "df1['id'] = df1['id'].astype(\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:30.713810Z",
     "start_time": "2020-10-15T00:21:30.658835Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Descriptive Statistical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1. Numerical Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A descriptive statistic is a summary statistic that quantitatively describes or summarizes features from a collection of information. Some measures that are commonly used to describe a dataset are measures of central tendency and measures of variability or dispersion. Measures of central tendency include the mean, median and mode, while measures of variability include the standard deviation (or variance), the minimum and maximum values of the variables, kurtosis and skewness. But, let's understand what really is skewness and kurtosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Skewness is a measure of symmetry, or more precisely, the lack of symmetry. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with high kurtosis tend to have heavy tails, or outliers. Data sets with low kurtosis tend to have light tails, or lack of outliers. A uniform distribution would be the extreme case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:31.754735Z",
     "start_time": "2020-10-15T00:21:30.902252Z"
    }
   },
   "outputs": [],
   "source": [
    "#filtering all the numerical data\n",
    "num_attributes = df1.select_dtypes(exclude = ['object'])\n",
    "\n",
    "#dropping the binary features\n",
    "num_attributes.drop(columns = ['gluc','smoke','alco','active','cardio','cholesterol'], axis = 1, inplace = True)\n",
    "\n",
    "#central tendency - mean, median\n",
    "ct1 = pd.DataFrame(num_attributes.apply(np.mean)).T\n",
    "ct2 = pd.DataFrame(num_attributes.apply(np.median)).T\n",
    "\n",
    "#dipersion - std,min,max, range, skew, kurtosis\n",
    "d1 = pd.DataFrame(num_attributes.apply(np.std)).T\n",
    "d2 = pd.DataFrame(num_attributes.apply(min)).T\n",
    "d3 = pd.DataFrame(num_attributes.apply(max)).T\n",
    "d4 = pd.DataFrame(num_attributes.apply(lambda x: x.max() - x.min())).T\n",
    "d5 = pd.DataFrame(num_attributes.apply(lambda x: x.skew())).T\n",
    "d6 = pd.DataFrame(num_attributes.apply(lambda x: x.kurtosis())).T\n",
    "\n",
    "#concat as a unique dataframe\n",
    "desc_stats = pd.concat([d2,d3,d4,ct1,ct2,d1,d5,d6]).T.reset_index()\n",
    "columns = ['attributes','min','max','range','mean','median','std','skew','kurtosis']\n",
    "\n",
    "desc_stats.columns = columns\n",
    "desc_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this table, we can get some informations:\n",
    "\n",
    "- There aren't children included in the dataset\n",
    "\n",
    "- The minimum value for the \"height\" column is 55, but the minimum value for the age_year column is 29. As we said, we don't have children included in the data set\n",
    "\n",
    "- \"ap_hi\" and \"ap_lo have a high skew, this probably means that we have a non-symmetric graph shifted to the right.\n",
    "\n",
    "- If we remember that a person who has systolic blood pressure of 180mmHg must urgently go to the doctor and with a diastolic pressure greater than 120mmHg. However, we have as maximum values in the columns \"ap_hi\" and \"ap_lo\" the values 16020.0 and 11000.0 respectively. This is a strange value that needs to be analyzed soon, but it could indicates that it is an outlier.\n",
    "\n",
    "- There are also negative values for the columns \"ap_hi\" and \"ap_lo\", this represents an inconsistent value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2. Binary and Ordinal Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:34.306334Z",
     "start_time": "2020-10-15T00:21:31.763092Z"
    }
   },
   "outputs": [],
   "source": [
    "#filtering each binary attributes from our original dataframe\n",
    "binary_attributes = df1[['gluc','smoke','alco','active','cardio','cholesterol']]\n",
    "\n",
    "#plotting each feature from binary_attributes dataframe\n",
    "plt.subplot(2,3,1)\n",
    "sns.countplot(binary_attributes['gluc'])\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "sns.countplot(binary_attributes['smoke'])\n",
    "\n",
    "plt.subplot(2,3,3)\n",
    "sns.countplot(binary_attributes['alco'])\n",
    "\n",
    "plt.subplot(2,3,4)\n",
    "sns.countplot(binary_attributes['active'])\n",
    "\n",
    "plt.subplot(2,3,5)\n",
    "sns.countplot(binary_attributes['cardio'])\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "sns.countplot(binary_attributes['cholesterol'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these plots we can get some informations:\n",
    "\n",
    "**1.** There is much more data from people with normal glucose levels.\n",
    "\n",
    "**2.** Our target variable has balanced data, which is very good, it will avoid a lot of work.\n",
    "\n",
    "**3.** The number of people who do physical activities is considerably greater than those who don't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0. FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A feature could be strongly relevant, the feature has information that doesn't exist in any other feature, relevant, weakly relevant (some information that other features include) or irrelevant. Even if some features are irrelevant, having too many is better than missing those that are important. \n",
    "\n",
    "The feature engineering process is:\n",
    "\n",
    "- Creating a mind map hypothesis;\n",
    "- Deciding what features to create;\n",
    "- Creating features;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:34.319201Z",
     "start_time": "2020-10-15T00:21:34.309948Z"
    }
   },
   "outputs": [],
   "source": [
    "#making a copy of the dataset\n",
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. MindMap Hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:34.509499Z",
     "start_time": "2020-10-15T00:21:34.322810Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "Image(\"img/MindMapHypothesis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Hypothesis List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** **AGE**\n",
    "   - There are more cases of cardiovascular disease in older people.\n",
    "    \n",
    "**2.** **GENDER**\n",
    "   - The presence of cardiovascular diseases will not depend directly on gender.\n",
    "    \n",
    "**3.** **BLOOD PRESSURE**\n",
    "   - There are more cases of cardiovascular disease in people with high diastolic and systolic pressure.\n",
    "    \n",
    "**4.** **SMOKING**\n",
    "   - There are more cases of cardiovascular disease in people who smoke.\n",
    "    \n",
    "**5.** **ALCOHOL INTAKE**\n",
    "   - There are more cases of cardiovascular disease in people who drink alcoholic beverages\n",
    "    \n",
    "**6.** **WEIGHT**\n",
    "   - There are more cases of cardiovascular disease in people who are overweight.\n",
    "\n",
    "**7.** **PHYSICAL ACTIVITY**\n",
    "   - There are fewer cases of cardiovascular disease in people who practice physical activity\n",
    "    \n",
    "**8.** **GLUCOSE**\n",
    "   - There are more cases of cardiovascular disease in people who have a high glucose rate.\n",
    "    \n",
    "**9.** **CHOLESTEROL**\n",
    "   - There is a incresed risk of cardiovascular disease if your cholesterol is above normal.\n",
    "   \n",
    "**10.** **HEIGHT**\n",
    "   - The presence of cardiovascular diseases is not influenced by height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Blood Pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a new variable to make it easier to visualize in the exploratory data analysis stage. This feature will be derived from our systolic and diastolic pressure values. We will classify them into groups as shown below:\n",
    "\n",
    "   - Normal -> Systolic Less than 120 and diastolic less than 80\n",
    "   - Elevated -> Systolic between 120-129 and diastolic less than 80\n",
    "   - High Blood Pressure -> Systolic between 130-139 and diastolic between 80-89\n",
    "   - Hypertension -> Systolic above the 140 and diastolic equal or above the 90."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:39.907041Z",
     "start_time": "2020-10-15T00:21:34.513643Z"
    }
   },
   "outputs": [],
   "source": [
    "#blood pressure\n",
    "df2['blood_pressure'] = df2.apply(lambda x:'normal' if (x['ap_hi'] <= 120) & (x['ap_lo'] <= 80)\n",
    "                                 else 'elevated' if (x['ap_hi'] > 120 and x['ap_hi'] <= 129) & (x['ap_lo'] <= 80)\n",
    "                                 else \"high_blood_pressure\" if (x['ap_hi'] >= 130 and x['ap_hi'] <= 139) & (x['ap_lo'] <= 90)\n",
    "                                 else 'hypertension', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overweight and obesity are defined by the World Health Organization as abnormal or excessive fat that accumulate and present a risk to health. Obesity is measured in body mass index (BMI), which is a person’s weight (in kilograms) divided by the square of his or her height (in meters). A person with a BMI of 30 or more is generally considered obese. A person with a BMI equal to or more than 25 is considered overweight.\n",
    "\n",
    "Therefore, the creation of the variable \"bmi\" will be more faithful than just the variable \"weight\", because sometimes we have people with high weight but with high height. So, the weight is directly proportional to the height.\n",
    "\n",
    "\n",
    "**BMI = Weight(kg) / (Height)²(m²)**\n",
    "\n",
    "Reference: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3250069/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our \"height\" column is in centimeters, we will have to convert it to square meter. Therefore, we will divide by 10,000\n",
    "\n",
    "**m² = cm²/10000**\n",
    "\n",
    "Reference: https://www.asknumbers.com/square-centimeter-to-square-meter.aspx#:~:text=1%20Square%20meter%20(m2)%20is,100000%20cm2%20is%2010%20m2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:39.927018Z",
     "start_time": "2020-10-15T00:21:39.916594Z"
    }
   },
   "outputs": [],
   "source": [
    "#bmi calculation\n",
    "df2['bmi'] = round((df2['weight']/((df2['height'] * df2['height'])/10000)),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we will derive our feature \"status_bmi\" which will be represented according to the image below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:40.504234Z",
     "start_time": "2020-10-15T00:21:40.332925Z"
    }
   },
   "outputs": [],
   "source": [
    "#creating status_bmi \n",
    "df2['status_bmi'] = df2['bmi'].apply(lambda x: 'underweight' if x <= 18.5\n",
    "                                          else 'normal' if (x > 18.5) & (x < 24.9) \n",
    "                                          else 'overweight' if (x >= 25) & (x <= 29.9)\n",
    "                                          else 'obse' if (x >= 30) & (x<= 34.9) \n",
    "                                          else 'extremely_obese')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build groups for the ages to be easy in the hypothesis validation stage.\n",
    "\n",
    "The groupings will be:\n",
    "- 0-50\n",
    "- 50-65\n",
    "- above 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:40.576561Z",
     "start_time": "2020-10-15T00:21:40.508885Z"
    }
   },
   "outputs": [],
   "source": [
    "#creating age rage\n",
    "df2['age_range'] = df2['age_year'].apply(lambda x:\"0-50\" if x <= 50\n",
    "                                             else \"50-65\" if (x>50) & (x<=65)\n",
    "                                             else \">65\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0. DATA FILTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:40.666830Z",
     "start_time": "2020-10-15T00:21:40.583112Z"
    }
   },
   "outputs": [],
   "source": [
    "#making a new copy of the dataframe\n",
    "df3 = df2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Filtering Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will remove the columns. These will be:\n",
    "\n",
    "1. **id** - This column is just a unique identifier for each person. Therefore, it will not be interesting for our model.\n",
    "\n",
    "2. **age** - This column is in the format of days, as we built another column with the age in years, it would be redundant to keep both, because we will have the same information in different formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:40.757648Z",
     "start_time": "2020-10-15T00:21:40.674846Z"
    }
   },
   "outputs": [],
   "source": [
    "cols_drop = ['age','id']\n",
    "df3.drop(columns = cols_drop, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Filtering Rows --> Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:40.829606Z",
     "start_time": "2020-10-15T00:21:40.761975Z"
    }
   },
   "outputs": [],
   "source": [
    "#copying a new dataframe\n",
    "df3_clean = df3.copy() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the inconsistencies in the following columns:\n",
    "\n",
    "- ap_hi\n",
    "- ap_lo\n",
    "- height\n",
    "- weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Blood Pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1.1 Systolic Blood Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:42.138721Z",
     "start_time": "2020-10-15T00:21:40.833874Z"
    }
   },
   "outputs": [],
   "source": [
    "#boxplot no filter\n",
    "plt.subplot(1,2,1)\n",
    "sns.boxplot(df3_clean['ap_hi'])\n",
    "plt.title(\"No Filter\", fontsize = 25)\n",
    "\n",
    "#boxplot with filter\n",
    "plt.subplot(1,2,2)\n",
    "aux1 = df3_clean[df3_clean['ap_hi'] < 200]\n",
    "sns.boxplot(aux1['ap_hi'])\n",
    "plt.title(\"With Filter\", fontsize = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to keep some values outside the 3rd quartile because a patient with a blood pressure of 190/90 mmHg or 175/115 has a hypertensive crisis. The higher the blood pressure, the more severe the crisis. Some patients even have 240 or 250 mmHg of maximum pressure during a hypertensive peak.\n",
    "\n",
    "From the boxplot, we can identify a large presence of outliers, even applying a filter for better visualization of the quartiles. Therefore, analyzing statistically, we will remove the following values:\n",
    "\n",
    "- **Systolic** <= 90 and **Systolic** >= 220"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1.2 Diastolic Blood Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:43.186158Z",
     "start_time": "2020-10-15T00:21:42.142694Z"
    }
   },
   "outputs": [],
   "source": [
    "#boxplot no filter\n",
    "plt.subplot(1,2,1)\n",
    "sns.boxplot(df3_clean['ap_lo'])\n",
    "plt.title(\"No Filter\", fontsize = 25)\n",
    "\n",
    "#boxplot with filter\n",
    "plt.subplot(1,2,2)\n",
    "aux1 = df3_clean[df3_clean['ap_lo'] < 200]\n",
    "sns.boxplot(aux1['ap_lo'])\n",
    "plt.title(\"With Filter\", fontsize = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the boxplot, we can identify a large presence of outliers, even applying a filter for better visualization of the quartiles. Therefore, analyzing statistically, we will remove the following values:\n",
    "\n",
    "- **Diastolic** <= 65 and **Diastolic** >= 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1.3 Removing Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the following values:\n",
    "\n",
    "- Systolic >= 210 and Diastolic >=150\n",
    "- Systolic <= 90 and Diastolic <= 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:43.215946Z",
     "start_time": "2020-10-15T00:21:43.189624Z"
    }
   },
   "outputs": [],
   "source": [
    "#removing outliers\n",
    "df3_clean = df3_clean[~((df3_clean['ap_hi'] >= 220) | (df3_clean['ap_hi'] <= 90) \n",
    "                        | (df3_clean['ap_lo'] >= 150) | (df3_clean['ap_lo'] <= 65))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:44.545456Z",
     "start_time": "2020-10-15T00:21:43.222589Z"
    }
   },
   "outputs": [],
   "source": [
    "#building a boxplot without outliers\n",
    "plt.subplot(1,2,1)\n",
    "sns.boxplot(df3_clean['ap_lo'])\n",
    "plt.title(\"Diastolic\", fontsize = 25)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.boxplot(df3_clean['ap_hi'])\n",
    "plt.title(\"Systolic\", fontsize = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, there is some data that is outside the boxplot. However, we will consider, therefore, there are cases in which, due to systemic problems, this variation can happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Height "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted in the descriptive analysis, we have some inconsistencies in our height variable that needed to be checked. Among them, a person who had 55cm, this is extremely unusual. Therefore, after some research, I found that the height of a dwarf is approximately 140 cm,so, if we don't remove these values that are out of reality, we could add a bias that we don't want. We also found that the height of the tallest human in the world is 246cm, which is beyond normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:47.361167Z",
     "start_time": "2020-10-15T00:21:44.550911Z"
    }
   },
   "outputs": [],
   "source": [
    "#boxplot no filter\n",
    "plt.subplot(1,2,1)\n",
    "sns.boxplot(df3_clean['height'])\n",
    "plt.title(\"Height\", fontsize = 25)\n",
    "\n",
    "#distribution plot\n",
    "plt.subplot(1,2,2)\n",
    "sns.distplot(df3_clean['height'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we will consider the following values:\n",
    "\n",
    "- values > 140cm\n",
    "- values < 215cm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:47.389199Z",
     "start_time": "2020-10-15T00:21:47.365142Z"
    }
   },
   "outputs": [],
   "source": [
    "#filtering values\n",
    "df3_clean = df3_clean[(df3_clean['height'] > 140) & (df3_clean['height'] < 220)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:49.964885Z",
     "start_time": "2020-10-15T00:21:47.393730Z"
    }
   },
   "outputs": [],
   "source": [
    "#boxplot\n",
    "plt.subplot(1,2,1)\n",
    "sns.boxplot(df3_clean['height'])\n",
    "\n",
    "#distribuition plot\n",
    "plt.subplot(1,2,2)\n",
    "sns.distplot(df3_clean['height'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the outliers, we obtain a distribution similar to Gaussian, but with more oscillations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3. Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our goal is to forecast medical diagnoses, for safety reasons, we will remove the values of the weight variable that are less than 43kg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:50.674927Z",
     "start_time": "2020-10-15T00:21:49.968635Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.boxplot(df3_clean['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:21:51.386029Z",
     "start_time": "2020-10-15T00:21:50.688105Z"
    }
   },
   "outputs": [],
   "source": [
    "#removing inconsistent values.\n",
    "df3_clean = df3_clean[~(df3_clean['weight'] < 43)]\n",
    "\n",
    "sns.boxplot(df3_clean['weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 4.0. EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Exploratory Data Analysis (EDA) is the process of visualizing and analyzing data to extract insights from it. In other words, EDA is the process of summarizing important characteristics of data in order to gain better understanding of the dataset. Therefore, this part will be done in the following three steps:\n",
    "\n",
    "- Univariate Analysis\n",
    "- Bivariate Analysis\n",
    "- Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:22:27.144873Z",
     "start_time": "2020-10-15T00:22:27.134199Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df4 = df3_clean.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 4.1. Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 4.1.1 Response Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T15:05:30.329315Z",
     "start_time": "2020-09-30T15:05:29.495238Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#building a countplot to our response variable\n",
    "sns.countplot(df4['cardio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As we can see, we have our classes balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 4.1.2 Numerical Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T15:05:30.343676Z",
     "start_time": "2020-09-30T15:05:30.332405Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#selecting numerical variables\n",
    "num_attributes = df4.select_dtypes(exclude = ['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T15:05:35.281367Z",
     "start_time": "2020-09-30T15:05:30.346013Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#plotting histogram\n",
    "num_attributes.hist(bins = 25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "From the graphs above, we can get some conclusions:\n",
    "\n",
    "- **gender** \n",
    "    - Our class that has the gender \"1\" has almost twice the gender class \"2\".\n",
    "- **height**\n",
    "    - It has a distribution similar to a normal one.\n",
    "- **weight** \n",
    "    - It has a distribution similar to a normal one.\n",
    "- **bmi**\n",
    "    - It has a distribution similar to a normal one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 4.1.3 Categorical Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this subsection, we will focus on analyzing our categorical variables and which have some qualitative interpretation, such as our \"cholesterol\" variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T15:05:35.294275Z",
     "start_time": "2020-09-30T15:05:35.284328Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#selecting \"categorical\" variables\n",
    "cat_attributes = df4[['cholesterol','gluc','smoke','alco','active','blood_pressure','status_bmi','age_range','gender']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T15:05:43.409257Z",
     "start_time": "2020-09-30T15:05:35.297674Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#plotting each variable\n",
    "plt.subplot(3,3,1)\n",
    "sns.countplot(cat_attributes['cholesterol'])\n",
    "\n",
    "plt.subplot(3,3,2)\n",
    "sns.countplot(cat_attributes['gluc'])\n",
    "\n",
    "plt.subplot(3,3,3)\n",
    "sns.countplot(cat_attributes['smoke'])\n",
    "\n",
    "plt.subplot(3,3,4)\n",
    "sns.countplot(cat_attributes['alco'])\n",
    "\n",
    "plt.subplot(3,3,5)\n",
    "sns.countplot(cat_attributes['active'])\n",
    "\n",
    "plt.subplot(3,3,6)\n",
    "sns.countplot(cat_attributes['blood_pressure'])\n",
    "\n",
    "plt.subplot(3,3,7)\n",
    "sns.countplot(cat_attributes['status_bmi'])\n",
    "\n",
    "plt.subplot(3,3,8)\n",
    "sns.countplot(cat_attributes['age_range'])\n",
    "\n",
    "plt.subplot(3,3,9)\n",
    "sns.countplot(cat_attributes['gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    " From these plots, we've got some informations:\n",
    " \n",
    "- The number of people with normal weight and overweight are very close. The number of obese people is half the number of people with normal weight.\n",
    " \n",
    "- There are twice as many people between 50-65 than 0-50.\n",
    " \n",
    "- There are almost twice as many people of gender 1 compared to 2.\n",
    " \n",
    "- The number of hypertensive people is half that of normal people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 4.2. Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T12:06:58.629453Z",
     "start_time": "2020-09-28T12:06:58.626012Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4.2.1 Final Hypothesis List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**1.** **AGE**\n",
    "   - There are more cases of cardiovascular disease in older people.\n",
    "    \n",
    "**2.** **GENDER**\n",
    "   - The presence of cardiovascular diseases will not depend directly on gender.\n",
    "    \n",
    "**3.** **BLOOD PESSURE**\n",
    "   - There are more cases of cardiovascular disease in people with high diastolic and systolic pressure.\n",
    "    \n",
    "**4.** **SMOKING**\n",
    "   - There are more cases of cardiovascular disease in people who smoke.\n",
    "    \n",
    "**5.** **ALCOHOL INTAKE**\n",
    "   - There are more cases of cardiovascular disease in people who drink alcoholic beverages\n",
    "    \n",
    "**6.** **WEIGHT**\n",
    "   - There are more cases of cardiovascular disease in people who are overweight.\n",
    "\n",
    "**7.** **PHYSICAL ACTIVITY**\n",
    "   - There are fewer cases of cardiovascular disease in people who practice physical activity\n",
    "    \n",
    "**8.** **GLUCOSE**\n",
    "   - There are more cases of cardiovascular disease in people who have a high glucose rate.\n",
    "    \n",
    "**9.** **CHOLESTEROL**\n",
    "   - There is a incresed risk of cardiovascular disease if your cholesterol is above normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### H1. There are more cases of cardiovascular diseases in older people.\n",
    "**True** It is true until the age of 59, after that, there is a drop in the number of people with cardiovascular diseases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T00:34:27.503958Z",
     "start_time": "2020-10-13T00:34:25.719439Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux1 = df4[df4['cardio'] == 1]\n",
    "aux2 = aux1[['age_year','cardio','age_range']].groupby([\"age_year\",'age_range']).count().reset_index()\n",
    "\n",
    "#plot 1 - age x cardio (bar plot)\n",
    "plt.subplot(1,3,1)\n",
    "sns.barplot(x = 'age_year', y = 'cardio', data = aux2)\n",
    "\n",
    "#plot2 - age x cardio (regression plot)\n",
    "plt.subplot(1,3,2)\n",
    "sns.regplot(x = 'age_year', y = 'cardio', data = aux2)\n",
    "\n",
    "#plot3 - age x cardio (correlation plot)\n",
    "plt.subplot(1,3,3)\n",
    "sns.heatmap(df4[['age_year','cardio']].corr(method = 'pearson'), annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "From these plots, we can get some informations:\n",
    "\n",
    "- There is a growing trend between age and the number of people with cardiovascular disease.\n",
    "- After 59 years, we can see a drop in the number of people with cardiovascular disease.\n",
    "- The \"age\" variable has a low correlation with our target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### H2. The presence of cardiovascular diseases will not depend directly on gender.\n",
    "**False** There are twice as many cases of cardiovascular disease in people with gender equal 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T15:05:46.587130Z",
     "start_time": "2020-09-30T15:05:45.523632Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux1 = df4[df4['cardio'] == 1]\n",
    "aux2 = aux1[['gender','cardio']].groupby(\"gender\").count().reset_index()\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "sns.countplot(df4['gender'])\n",
    "plt.title(\"Gender\",fontsize = 25)\n",
    "\n",
    "#plot2 - gender x cardio (bar plot)\n",
    "plt.subplot(1,2,2)\n",
    "sns.barplot(x = 'gender', y = 'cardio', data = aux2)\n",
    "plt.title(\"People who have cardiovascular diseases\", fontsize = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### H3. There are more cases of cardiovascular disease in people with high diastolic and systolic pressure.\n",
    "**True** Although the number of people with hypertension is higher, there are more cases of people with normal blood pressure than with high blood pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T15:05:46.936562Z",
     "start_time": "2020-09-30T15:05:46.590697Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#filtering peoples with cardiovascular diseases\n",
    "aux1 = df4[df4['cardio'] == 1]\n",
    "aux2 = aux1[['blood_pressure','cardio']].groupby('blood_pressure').count().reset_index()\n",
    "\n",
    "aux3 = aux1[['ap_hi','cardio']].groupby(\"ap_hi\").count().reset_index()\n",
    "aux4 = aux1[['ap_lo','cardio']].groupby(\"ap_lo\").count().reset_index()\n",
    "\n",
    "#plot1 - blood_pressure x Cardio (barplot)\n",
    "sns.barplot(x = 'blood_pressure', y = 'cardio', data = aux2)\n",
    "plt.title(\"Blood Pressure x CVD\", fontsize = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T15:05:49.015450Z",
     "start_time": "2020-09-30T15:05:46.939579Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#plo1 - systolic blood pressure x cardio (scatterplot/dipersion plot)\n",
    "plt.subplot(2,2,1)\n",
    "sns.scatterplot(x = 'ap_hi', y  = 'cardio', data = aux3)\n",
    "plt.title(\"Systolic Blood Pressure\", fontsize = 20)\n",
    "\n",
    "#plot2 - check the correlation between binary variables(cardio) and continuous variables(ap_hi) (point biseral)\n",
    "plt.subplot(2,2,2)\n",
    "point_bi(df4[['cardio']],df4[['ap_hi']])\n",
    " \n",
    "#plot3 - diastolic blood pressure x cardio (scatterplot / dispersion plot)    \n",
    "plt.subplot(2,2,3)\n",
    "sns.scatterplot(x = 'ap_lo', y  = 'cardio', data = aux4)\n",
    "plt.title(\"Diastolic Blood Pressure\", fontsize = 20)\n",
    "\n",
    "#plot4 - check the correlatio between binary variables(cardio) and continuous variables(ap_lo) (point biseral)\n",
    "plt.subplot(2,2,4)\n",
    "point_bi(df4[['cardio']],df4[['ap_lo']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What can we get about the plots above?\n",
    "\n",
    "- There are many people with hypertension and cardiovascular disease. However, there are also a lot of people without hypertension and with cardiovascular disease.\n",
    "- In scatterplots, we can see some points with a very high number of cases of cardiovascular diseases. For example, there are almost 10,000 people with 120mmHg of systolic blood pressure with CVD.\n",
    "- In both types of blood pressure, we have a positive correlation with our target variable. Systolic blood pressure has a moderate correlation and diastolic blood, a weak correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### H4. There are more cases of cardiovascular disease in people who smoke.\n",
    "**False** There are more cases in people who don't smoke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T15:05:50.042236Z",
     "start_time": "2020-09-30T15:05:49.018579Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux1 = df4[df4['cardio'] == 1]\n",
    "aux2 = aux1[['smoke','cardio']].groupby(\"smoke\").count().reset_index()\n",
    "\n",
    "#plot1 - countplot to our smoke variable\n",
    "plt.subplot(1,2,1)\n",
    "sns.countplot(df4['smoke'])\n",
    "plt.title(\"Number of people who smoke\",fontsize = 25)\n",
    "\n",
    "#plot2 - Cvd x Smoke (barplot)\n",
    "plt.subplot(1,2,2)\n",
    "sns.barplot(x = 'smoke', y = 'cardio', data = aux2)\n",
    "plt.title(\"CVD x Smoke\", fontsize = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "From these plots, we can get some informations:\n",
    "\n",
    "- The number of people who do not smoke is much greater than those who smoke.\n",
    "- The number of people who do not smoke and have cardiovascular disease is much greater than those who smoke.\n",
    "- The explanation of the second point described above, perhaps can be explained by the first point. We can't be sure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### H5. There are more cases of cardiovascular disease in people who drink alcoholic beverages\n",
    "**False** There are more cases in people who don't drink alcohol beverages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T15:05:50.948200Z",
     "start_time": "2020-09-30T15:05:50.045040Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux1 = df4[df4['cardio'] == 1]\n",
    "aux2 = aux1[['alco','cardio']].groupby(\"alco\").count().reset_index()\n",
    "\n",
    "#plo1 - countplot to our alcohol variable \n",
    "plt.subplot(1,2,1)\n",
    "sns.countplot(df4['alco'])\n",
    "plt.title(\"Alcohol\", fontsize = 25)\n",
    "\n",
    "#plot2 - alcohol x cardio (barplot)\n",
    "plt.subplot(1,2,2)\n",
    "sns.barplot(x = 'alco', y = 'cardio', data = aux2)\n",
    "plt.title(\"Alcohol and Cardio\",fontsize = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "From these plots above, we can get some informations:\n",
    "\n",
    "- The number of people who do not consume alcoholic beverages is much greater than those who do.\n",
    "- The number of people who do not consume alcoholic beverages and who have cardiovascular disease is much greater than the people who consume alcoholic beverages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### H6. There are more cases of cardiovascular disease in people who are overweight.\n",
    "**True** However, the number of cases in obese people is less than the number of cases of people with normal weight. The number of extremely obese people with cardiovascular disease is almost half the number of people with a normal person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T15:05:52.519602Z",
     "start_time": "2020-09-30T15:05:50.951263Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux1 = df4[df4['cardio'] == 1]\n",
    "aux2 = aux1[['status_bmi','cardio']].groupby(\"status_bmi\").count().reset_index()\n",
    "aux3 = aux1[['bmi','cardio']].groupby(\"bmi\").count().reset_index()\n",
    "\n",
    "#plot1 - BMI X CARDIO (BARPLOT)\n",
    "plt.subplot(1,3,1)\n",
    "sns.barplot(x = 'status_bmi', y = 'cardio', data = aux2)\n",
    "plt.title(\"BMI x CVD\",fontsize = 25)\n",
    "\n",
    "#plot2 - BMI X CVD (SCATTERPLOT/DISPERSION PLOT)\n",
    "plt.subplot(1,3,2)\n",
    "sns.scatterplot(x = 'bmi', y = 'cardio', data = aux3)\n",
    "plt.title(\"BMI x CVD\",fontsize = 25)\n",
    "\n",
    "#plot3 - Correlation between binary variable(cardio) and continuous variable(bmi) using point biseral.\n",
    "plt.subplot(1,3,3)\n",
    "point_bi(df4[['cardio']], df4[['bmi']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- The number of cases in obese people is less than the number of cases of people with normal weight. \n",
    "- The number of extremely obese people with cardiovascular disease is almost half the number of people with a normal person.\n",
    "- There's a low positive correlation between bmi and cardio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### H7. There are fewer cases of cardiovascular disease in people who practice physical activity\n",
    "**True** There are more cases of cardiovascular diseases in people who don't practice physical activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T15:05:53.787654Z",
     "start_time": "2020-09-30T15:05:52.522741Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#plot - countplot to our active variable\n",
    "sns.countplot(x = 'active', hue = 'cardio', data = df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "About the plots above, we can notice some points:\n",
    "\n",
    "- For people who do not practice physical activity, there is a greater number of cardiovascular diseases.\n",
    "- For people who practice physical activity, there are fewer people with cardiovascular disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### H8. There are more cases of cardiovascular disease in people who have a high glucose rate.\n",
    "**True** There are more cases of cardiovascular diseases in people who have a high glucose rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T15:05:55.166680Z",
     "start_time": "2020-09-30T15:05:53.790406Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#plot - Compare glucose level with the presence of CVD.\n",
    "sns.countplot(x = 'gluc', hue = 'cardio', data = df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- There are more cases of cardiovascular diseases in people who have a high glucose rate.\n",
    "- There are many more people with normal glucose levels, but a high number of people with CVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### H9. There is a incresed risk of cardiovascular disease if your cholesterol is above normal.\n",
    "**True** There are more cases of cardiovascular diseases in people who have high cholesterol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T15:05:56.344635Z",
     "start_time": "2020-09-30T15:05:55.168892Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(x = 'cholesterol', hue = 'cardio', data = df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "From these plots we can get some informations:\n",
    "\n",
    "- There is a high number of people with a normal cholesterol level but with the presence of cardiovascular diseases. Although the quantity is smaller, it is a high number that needs to be observed.\n",
    "- The level of people with high cholesterol who have CVD is higher than those who are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 4.2.2 Summary of Hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T20:19:41.928092Z",
     "start_time": "2020-10-11T20:19:41.919958Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tab = [['Hypotheses', 'Conclusion', 'Relevance'],\n",
    "       ['H1', 'True', 'Medium'],\n",
    "       ['H2', 'False', 'Low'],\n",
    "       ['H3', 'True', 'High'],\n",
    "       ['H4', 'False', 'Low'],\n",
    "       ['H5', 'False', 'Low'],\n",
    "       ['H6', 'True', 'High'],\n",
    "       ['H7', 'False', 'Medium'],\n",
    "       ['H8', 'True', 'Medium'],\n",
    "       ['H9', 'True', 'High']]\n",
    "\n",
    "print(tabulate(tab, headers = 'firstrow'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 4.3. Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T17:39:07.096188Z",
     "start_time": "2020-09-28T17:39:05.157237Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#exclude numerical variables\n",
    "aux1 = df4.select_dtypes(exclude = ['object'])\n",
    "\n",
    "#plot correlation plot to our numerical variables.\n",
    "sns.heatmap(aux1.corr(method = 'pearson'), annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0. BALANCED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:22:31.699378Z",
     "start_time": "2020-10-15T00:22:31.683735Z"
    }
   },
   "outputs": [],
   "source": [
    "df5 = df4.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Random Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T00:36:21.114300Z",
     "start_time": "2020-10-06T00:36:20.941954Z"
    }
   },
   "outputs": [],
   "source": [
    "#define sampler\n",
    "undersampling = us.RandomUnderSampler(sampling_strategy = \"majority\", random_state = 32)\n",
    "\n",
    "\n",
    "#apply sampler\n",
    "x_under, y_under = undersampling.fit_resample(df5 , df5['cardio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T00:36:21.463067Z",
     "start_time": "2020-10-06T00:36:21.452264Z"
    }
   },
   "outputs": [],
   "source": [
    "df5['cardio'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T00:36:21.915160Z",
     "start_time": "2020-10-06T00:36:21.902465Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_under.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T00:36:23.866037Z",
     "start_time": "2020-10-06T00:36:23.646232Z"
    }
   },
   "outputs": [],
   "source": [
    "#define sampler\n",
    "oversampling = oversamp.RandomOverSampler(sampling_strategy = 'minority', random_state = 32)\n",
    "\n",
    "#apply sampler\n",
    "x_over, y_over = oversampling.fit_resample(df5 , df5['cardio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T00:36:24.262299Z",
     "start_time": "2020-10-06T00:36:24.255529Z"
    }
   },
   "outputs": [],
   "source": [
    "df5['cardio'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T00:36:24.707005Z",
     "start_time": "2020-10-06T00:36:24.700425Z"
    }
   },
   "outputs": [],
   "source": [
    "y_over.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. SMOTE + TOMEKLINK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:23:48.774373Z",
     "start_time": "2020-10-15T00:23:48.651394Z"
    }
   },
   "outputs": [],
   "source": [
    "#blood_pressure - OrdinalEncoding\n",
    "dict_blood = {'normal':1, 'elevated':2,'high_blood_pressure':3, 'hypertension':4}\n",
    "df5['blood_pressure'] = df5['blood_pressure'].map(dict_blood)\n",
    "\n",
    "#status_bmi - OrdinalEncoding\n",
    "dict_bmi = {'underweight':1,'normal':2,'overweight':3,'obse':4,'extremely_obese':5}\n",
    "df5['status_bmi'] = df5['status_bmi'].map(dict_bmi)\n",
    "\n",
    "#age_range - OrdinalEncoding\n",
    "dict_age_range = {'50-65':2,'0-50':1}\n",
    "df5['age_range'] = df5['age_range'].map(dict_age_range)\n",
    "\n",
    "#gender - OneHotEncoding\n",
    "df5['gender_01'] = df5['gender'].apply(lambda x: 1 if x == 1 else 0)\n",
    "df5['gender_02'] = df5['gender'].apply(lambda x: 1 if x == 2 else 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:23:57.852786Z",
     "start_time": "2020-10-15T00:23:49.100721Z"
    }
   },
   "outputs": [],
   "source": [
    "#define sampler\n",
    "smt = c.SMOTETomek(sampling_strategy = 'auto', random_state = 32, n_jobs = -1)\n",
    "\n",
    "#apply sampler\n",
    "x_smt, y_smt = smt.fit_resample(df5, df5['cardio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:23:58.363179Z",
     "start_time": "2020-10-15T00:23:57.858171Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(smt, open(\"./parameters/smt.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:23:58.510145Z",
     "start_time": "2020-10-15T00:23:58.367423Z"
    }
   },
   "outputs": [],
   "source": [
    "y_smt.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, we got our classes completely balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:23:58.622333Z",
     "start_time": "2020-10-15T00:23:58.513138Z"
    }
   },
   "outputs": [],
   "source": [
    "#join target variable\n",
    "df_smt = x_smt\n",
    "df_smt['cardio'] = y_smt\n",
    "df_smt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:23:58.764494Z",
     "start_time": "2020-10-15T00:23:58.625979Z"
    }
   },
   "outputs": [],
   "source": [
    "#Re-encoding\n",
    "#blood_pressure - OrdinalEncoding\n",
    "dict_blood = {1:'normal',2:'elevated',3:'high_blood_pressure',4:'hypertension'}\n",
    "df_smt['blood_pressure'] = df_smt['blood_pressure'].map(dict_blood)\n",
    "\n",
    "#status_bmi - OrdinalEncoding\n",
    "dict_bmi = {1:'underweight',2:'normal',3:'overweight',4:'obse',5:'extremely_obese'}\n",
    "df_smt['status_bmi'] = df_smt['status_bmi'].map(dict_bmi)\n",
    "\n",
    "#age_range - OrdinalEncoding\n",
    "dict_age_range = {2:'50-65',1:'0-50'}\n",
    "df_smt['age_range'] = df_smt['age_range'].map(dict_age_range)\n",
    "\n",
    "#gender - OneHotEncoding\n",
    "df_smt.drop(columns = ['gender_01','gender_02'],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:23:58.852166Z",
     "start_time": "2020-10-15T00:23:58.767509Z"
    }
   },
   "outputs": [],
   "source": [
    "df_smt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0. DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:23:58.946064Z",
     "start_time": "2020-10-15T00:23:58.860038Z"
    }
   },
   "outputs": [],
   "source": [
    "#making a copy of data\n",
    "df6 = df_smt.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Split dataframe into training and test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train-test split procedure is used to estimate the performance of machine learning algorithms when they are used to make predictions on data not used to train the model.\n",
    "It is procedure to perform, the results of which allow you to compare the performance of machine learning algorithms for your predictive modeling problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:23:59.057781Z",
     "start_time": "2020-10-15T00:23:58.950454Z"
    }
   },
   "outputs": [],
   "source": [
    "#split data\n",
    "X = df6.drop(['cardio',], axis = 1).copy()\n",
    "y = df6['cardio']\n",
    "\n",
    "#split data into training and test dataset\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the population mean and population standard deviation are known, a raw score x is converted into a standard score by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T20:30:08.392925Z",
     "start_time": "2020-10-11T20:30:03.378074Z"
    }
   },
   "outputs": [],
   "source": [
    "#height\n",
    "plt.subplot(2,3,1)\n",
    "sns.histplot(df6['height'])\n",
    "\n",
    "#weight\n",
    "plt.subplot(2,3,2)\n",
    "sns.histplot(df6['weight'])\n",
    "\n",
    "#ap_hi\n",
    "plt.subplot(2,3,3)\n",
    "sns.histplot(df6['ap_hi'])\n",
    "\n",
    "#ap_lo\n",
    "plt.subplot(2,3,4)\n",
    "sns.histplot(df6['ap_lo'])\n",
    "\n",
    "#age_year\n",
    "plt.subplot(2,3,5)\n",
    "sns.histplot(df6['age_year'])\n",
    "\n",
    "#bmi\n",
    "plt.subplot(2,3,6)\n",
    "sns.histplot(df6['bmi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- bmi -> It has a distribution close to normal.\n",
    "- weight -> It has a distribution close to normal, but with some oscillations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will perform the rescaling of our numerical variables. For this, we need to check the presence of outliers in order to choose the best technique for our model. The techniques that will be used:\n",
    "\n",
    "- RobustScaler -> Robust in the presence of outliers, therefore, in the variables that have a significant amount of outliers we will apply this technique.\n",
    "\n",
    "- MinMaxScaler -> Sensitive in the presence of outliers, therefore, we will apply to our variables that do not have outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T20:30:14.288395Z",
     "start_time": "2020-10-11T20:30:08.395597Z"
    }
   },
   "outputs": [],
   "source": [
    "#height\n",
    "plt.subplot(3,4,1)\n",
    "sns.boxplot(df6['height'])\n",
    "\n",
    "plt.subplot(3,4,2)\n",
    "sns.distplot(df6['height'])\n",
    "\n",
    "#weight\n",
    "plt.subplot(3,4,3)\n",
    "sns.boxplot(df6['weight'])\n",
    "\n",
    "plt.subplot(3,4,4)\n",
    "sns.distplot(df6['weight'])\n",
    "\n",
    "#ap_hi\n",
    "plt.subplot(3,4,5)\n",
    "sns.boxplot(df6['ap_hi'])\n",
    "\n",
    "plt.subplot(3,4,6)\n",
    "sns.distplot(df6['ap_hi'])\n",
    "\n",
    "#ap_lo\n",
    "plt.subplot(3,4,7)\n",
    "sns.boxplot(df6['ap_lo'])\n",
    "\n",
    "plt.subplot(3,4,8)\n",
    "sns.distplot(df6['ap_lo'])\n",
    "\n",
    "#age_year\n",
    "plt.subplot(3,4,9)\n",
    "sns.boxplot(df6['age_year'])\n",
    "\n",
    "plt.subplot(3,4,10)\n",
    "sns.distplot(df6['age_year'])\n",
    "\n",
    "#bmi\n",
    "plt.subplot(3,4,11)\n",
    "sns.boxplot(df6['bmi'])\n",
    "\n",
    "plt.subplot(3,4,12)\n",
    "sns.distplot(df6['bmi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RobustScaler \n",
    "    - ap_lo\n",
    "    - ap_hi\n",
    "    - bmi \n",
    "    - Weight\n",
    "\n",
    "- MinMaxScaler \n",
    "    - age_year\n",
    "    - height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:24:01.164159Z",
     "start_time": "2020-10-15T00:24:01.053250Z"
    }
   },
   "outputs": [],
   "source": [
    "rs = RobustScaler() #RobustScaler\n",
    "mms = MinMaxScaler() #MinMaxScaler\n",
    "\n",
    "#ap_lo - RobustScaler\n",
    "X_train['ap_lo'] = rs.fit_transform(X_train[['ap_lo']].values)\n",
    "X_test['ap_lo'] = rs.fit_transform(X_test[['ap_lo']].values)\n",
    "pickle.dump(rs, open(\"./parameters/rescaling_ap_lo.pkl\",\"wb\"))\n",
    "\n",
    "#ap_hi - RobustScaler\n",
    "X_train['ap_hi'] = rs.fit_transform(X_train[['ap_hi']].values)\n",
    "X_test['ap_hi'] = rs.fit_transform(X_test[['ap_hi']].values)\n",
    "pickle.dump(rs, open(\"./parameters/rescaling_ap_hi.pkl\",\"wb\"))\n",
    "\n",
    "#weight - RobustScaler\n",
    "X_train['weight'] = rs.fit_transform(X_train[['weight']].values)\n",
    "X_test['weight'] = rs.fit_transform(X_test[['weight']].values)\n",
    "pickle.dump(rs, open(\"./parameters/rescaling_weight.pkl\",\"wb\"))\n",
    "\n",
    "#bmi - RobustScaler\n",
    "X_train['bmi'] = rs.fit_transform(X_train[['bmi']].values)\n",
    "X_test['bmi'] = rs.fit_transform(X_test[['bmi']].values)\n",
    "pickle.dump(rs, open(\"./parameters/rescaling_bmi.pkl\",\"wb\"))\n",
    "\n",
    "#age_year - MinMaxScaler\n",
    "X_train['age_year'] = mms.fit_transform(X_train[['age_year']].values)\n",
    "X_test['age_year'] = mms.fit_transform(X_test[['age_year']].values)\n",
    "pickle.dump(mms, open(\"./parameters/rescaling_age_year.pkl\",\"wb\"))\n",
    "\n",
    "#height - MinMaxScaler \n",
    "X_train['height'] = mms.fit_transform(X_train[['height']].values)\n",
    "X_test['height'] = mms.fit_transform(X_test[['height']].values)\n",
    "pickle.dump(mms, open(\"./parameters/rescaling_height.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:24:02.347879Z",
     "start_time": "2020-10-15T00:24:02.249241Z"
    }
   },
   "outputs": [],
   "source": [
    "#blood_pressure - OrdinalEncoding\n",
    "dict_blood = {'normal':1, 'elevated':2,'high_blood_pressure':3, 'hypertension':4}\n",
    "X_train['blood_pressure'] = X_train['blood_pressure'].map(dict_blood)\n",
    "X_test['blood_pressure'] = X_test['blood_pressure'].map(dict_blood)\n",
    "\n",
    "#status_bmi - OrdinalEncoding\n",
    "dict_bmi = {'underweight':1,'normal':2,'overweight':3,'obse':4,'extremely_obese':5}\n",
    "X_train['status_bmi'] = X_train['status_bmi'].map(dict_bmi)\n",
    "X_test['status_bmi'] = X_test['status_bmi'].map(dict_bmi)\n",
    "\n",
    "#age_range - OrdinalEncoding\n",
    "dict_age_range = {'50-65':2,'0-50':1}\n",
    "X_train['age_range'] = X_train['age_range'].map(dict_age_range)\n",
    "X_test['age_range'] = X_test['age_range'].map(dict_age_range)\n",
    "\n",
    "#gender - OneHotEncoding\n",
    "X_train['gender_01'] = X_train['gender'].apply(lambda x: 1 if x == 1 else 0)\n",
    "X_train['gender_02'] = X_train['gender'].apply(lambda x: 1 if x == 2 else 0 )\n",
    "X_test['gender_01'] = X_test['gender'].apply(lambda x: 1 if x == 1 else 0)\n",
    "X_test['gender_02'] = X_test['gender'].apply(lambda x: 1 if x == 2 else 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:24:02.848059Z",
     "start_time": "2020-10-15T00:24:02.826038Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0. FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:24:03.697569Z",
     "start_time": "2020-10-15T00:24:03.688432Z"
    }
   },
   "outputs": [],
   "source": [
    "df6 = df5.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Boruta as Feature Selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Boruta algorithm is a wrapper built around the random forest classification algorithm. It tries to capture all the important, interesting features you might have in your dataset with respect to an outcome variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-02T03:39:51.070712Z",
     "start_time": "2020-10-02T03:39:08.408188Z"
    }
   },
   "outputs": [],
   "source": [
    "#define model\n",
    "xgb_model = xgb.XGBClassifier(n_jobs=-1)\n",
    "extra_model = ExtraTreesClassifier(n_jobs=-1)\n",
    "rf_model = RandomForestClassifier(n_jobs = -1)\n",
    "\n",
    "#split data \n",
    "X_train_nn = X_train.values\n",
    "y_train_nn = y_train.values.ravel()\n",
    "\n",
    "#define boruta\n",
    "boruta = BorutaPy(extra_model, n_estimators = 'auto', verbose = 2, random_state = 42).fit(X_train_nn, y_train_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-02T03:39:51.085950Z",
     "start_time": "2020-10-02T03:39:51.073998Z"
    }
   },
   "outputs": [],
   "source": [
    "#best features from boruta\n",
    "cols_selected = boruta.support_.tolist()\n",
    "cols_selected_boruta = X_train.iloc[:, cols_selected].columns.to_list()\n",
    "cols_selected_boruta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetures Selected From boruta\n",
    "\n",
    "- ap_hi \n",
    "- age_year\n",
    "- blood_pressure\n",
    "- bmi\n",
    "\n",
    "Obtaining the features selected by boruta, we will compare it with our relevance table that was built in the \"Exploratory Data Analysis\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Manual Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the best feature from boruta, we will based on our understanding of the business and add other variables.Therefore, although the algorithm is extremely important for selecting features, it does not nullify our opinion about the problem we are working on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:24:18.989939Z",
     "start_time": "2020-10-15T00:24:18.984869Z"
    }
   },
   "outputs": [],
   "source": [
    "#features that will be really important to our model\n",
    "cols_selected_boruta_full = ['ap_hi','ap_lo','age_year','weight','height','blood_pressure','bmi','cholesterol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:24:19.558601Z",
     "start_time": "2020-10-15T00:24:19.536514Z"
    }
   },
   "outputs": [],
   "source": [
    "#creating a dataframe with selected columns\n",
    "X_train_boruta = X_train[cols_selected_boruta_full].copy()\n",
    "X_test_boruta = X_test[cols_selected_boruta_full].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:24:24.828101Z",
     "start_time": "2020-10-15T00:24:20.210742Z"
    }
   },
   "outputs": [],
   "source": [
    "#feature importances \n",
    "rf_model = RandomForestClassifier(n_jobs = -1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "feat_importances = pd.Series(rf_model.feature_importances_, index=X_train.columns)\n",
    "feat_importances.sort_values().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.0. MACHINE LEARNING MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will finally build our predictive models. Therefore, we will use 5 machine learning algorithms, which will be:\n",
    "\n",
    "- Baseline Model\n",
    "- Logistic Regression\n",
    "- Support Vector Machine\n",
    "- XGBoost Classifier\n",
    "- K-Nearest Neighbors\n",
    "- LGBM Classifier\n",
    "- Random Forest Classifier\n",
    "\n",
    "For the best performing algorithm, we will build what we call the cross-validation technique. Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. So, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.\n",
    "\n",
    "It is a popular method because it is simple to understand and because it generally results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train / test split.\n",
    "\n",
    "For this, we will separate our predictor variables from our target variable and then separate them in training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:51:42.098985Z",
     "start_time": "2020-10-15T00:51:42.093329Z"
    }
   },
   "outputs": [],
   "source": [
    "#copying data\n",
    "X_train = X_train_boruta.copy()\n",
    "X_test = X_test_boruta.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:51:43.282077Z",
     "start_time": "2020-10-15T00:51:42.556874Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "cardio = y_test.drop_duplicates().sort_values().tolist()\n",
    "random_test = X_test.shape[0]\n",
    "cardio_weights = df1['cardio'].value_counts( normalize=True ).sort_index().tolist()\n",
    "\n",
    "# prediction\n",
    "yhat_random = random.choices( cardio, k=random_test,\n",
    "                              weights=cardio_weights )\n",
    "\n",
    "# Accuracy\n",
    "acc_random = accuracy_score( y_test, yhat_random )\n",
    "print( 'Accuracy: {}'.format( acc_random ) )\n",
    "\n",
    "# Kappa Metrics\n",
    "kappa_random = cohen_kappa_score( y_test, yhat_random )\n",
    "print( 'Kappa Score: {}'.format( kappa_random ) )\n",
    "\n",
    "# Classification report\n",
    "print( classification_report( y_test, yhat_random ) )\n",
    "\n",
    "# Confusion Matrix\n",
    "mt.plot_confusion_matrix( y_test, yhat_random, normalize=False, figsize=(12,12), cmap='plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:51:45.799254Z",
     "start_time": "2020-10-15T00:51:43.285049Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "#training model\n",
    "logreg.fit(X_train.values,y_train.values.ravel())\n",
    "\n",
    "#predict\n",
    "yhat_log = logreg.predict(X_test) \n",
    "\n",
    "# AUC-ROC\n",
    "logreg_cv = cross_val_predict(logreg, X_train, y_train, cv=5, method='decision_function')\n",
    "logreg_roc = roc_auc_score(y_train, logreg_cv)\n",
    "\n",
    "#performance\n",
    "result_log = pd.DataFrame([\"Logistic Regression\",accuracy_score(y_test,yhat_log),cohen_kappa_score(y_test,yhat_log),\n",
    "                          recall_score(y_test,yhat_log), f1_score(y_test,yhat_log),\n",
    "                           precision_score(y_test,yhat_log), logreg_roc]).T\n",
    "result_log.columns = [\"Model\",'Accuracy',\"Kappa Score\",\"Recall\",\"F1-Score\",\"Precision_score\",\"roc_auc_score\"]\n",
    "\n",
    "#classification_report\n",
    "print(classification_report(y_test, yhat_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:51:45.824345Z",
     "start_time": "2020-10-15T00:51:45.802958Z"
    }
   },
   "outputs": [],
   "source": [
    "#performance logistic regression\n",
    "result_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:51:46.862575Z",
     "start_time": "2020-10-15T00:51:45.827565Z"
    }
   },
   "outputs": [],
   "source": [
    "#confusion_matrix\n",
    "mt.plot_confusion_matrix(y_test,yhat_log, normalize = False, figsize = (12,12))\n",
    "\n",
    "#precision_recall_curve\n",
    "plot_precision_recall_curve(logreg, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1. Logistic Regression - Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:51:50.074281Z",
     "start_time": "2020-10-15T00:51:46.865328Z"
    }
   },
   "outputs": [],
   "source": [
    "#cross validation scores\n",
    "log_cv = cross_val_score(logreg,X_train, y_train.ravel(), cv = 10, n_jobs = -1)\n",
    "\n",
    "#mean and standard deviation\n",
    "print(\"Average: {} +/- Std {}\".format(np.round(np.mean(log_cv),4),np.round(np.std(log_cv),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:53:53.442138Z",
     "start_time": "2020-10-15T00:51:51.717727Z"
    }
   },
   "outputs": [],
   "source": [
    "#define model\n",
    "model_svm = svm.SVC()\n",
    "\n",
    "#training model\n",
    "model_svm.fit(X_train,y_train)\n",
    "\n",
    "#predict\n",
    "yhat_svm = model_svm.predict(X_test)\n",
    "\n",
    "#result_svm\n",
    "result_svm = pd.DataFrame([\"Support Vector Machine\",accuracy_score(y_test,yhat_svm),cohen_kappa_score(y_test,yhat_svm),\n",
    "                          recall_score(y_test,yhat_svm), f1_score(y_test,yhat_svm),roc_auc_score(y_test,yhat_svm),\n",
    "                          precision_score(y_test,yhat_svm)]).T\n",
    "result_svm.columns = [\"Model\",'Accuracy',\"Kappa Score\",\"Recall\",\"F1-Score\",\"roc_auc_score\",\"Precision_score\"]\n",
    "\n",
    "#performance\n",
    "print(classification_report(y_test, yhat_svm)) #classification report\n",
    "mt.plot_confusion_matrix(y_test, yhat_svm, normalize = False, figsize = (12,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:54:26.174448Z",
     "start_time": "2020-10-15T00:53:53.446294Z"
    }
   },
   "outputs": [],
   "source": [
    "#precision_recall_curve\n",
    "plot_precision_recall_curve(model_svm, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.1. Support Vector Machine - Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T20:47:22.756628Z",
     "start_time": "2020-10-11T20:35:00.713380Z"
    }
   },
   "outputs": [],
   "source": [
    "#cross validation scores\n",
    "svm_cv = cross_val_score(model_svm,X_train, y_train.ravel(), cv = 10, n_jobs = -1)\n",
    "\n",
    "#mean and standard deviation\n",
    "print(\"Average: {} +/- Std {}\".format(np.round(np.mean(svm_cv),4),np.round(np.std(svm_cv),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4. XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:54:43.450451Z",
     "start_time": "2020-10-15T00:54:26.178434Z"
    }
   },
   "outputs": [],
   "source": [
    "#model definition\n",
    "xgb_model = xgb.XGBClassifier(n_jobs = -1)\n",
    "\n",
    "#fit model\n",
    "xgb_model.fit(X_train,y_train)\n",
    "\n",
    "#prediction\n",
    "yhat_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# AUC-ROC\n",
    "xgb_cv = cross_val_predict(xgb_model, X_train, y_train, cv=5)\n",
    "xgb_roc = roc_auc_score(y_train, xgb_cv)\n",
    "\n",
    "\n",
    "#performance\n",
    "result_xgb = pd.DataFrame([\"XGBoost Classifier\",accuracy_score(y_test,yhat_xgb),cohen_kappa_score(y_test,yhat_xgb),\n",
    "                          recall_score(y_test,yhat_xgb), f1_score(y_test,yhat_xgb),xgb_roc,\n",
    "                          precision_score(y_test,yhat_xgb)]).T\n",
    "result_xgb.columns = [\"Model\",'Accuracy',\"Kappa Score\",\"Recall\",\"F1-Score\",\"roc_auc_score\",\"Precision_score\"]\n",
    "\n",
    "#classification_report\n",
    "print(classification_report(y_test, yhat_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:54:43.469993Z",
     "start_time": "2020-10-15T00:54:43.454184Z"
    }
   },
   "outputs": [],
   "source": [
    "#summary metrics\n",
    "result_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:54:44.652361Z",
     "start_time": "2020-10-15T00:54:43.473219Z"
    }
   },
   "outputs": [],
   "source": [
    "#confusion_matrix\n",
    "mt.plot_confusion_matrix(y_test,yhat_xgb, normalize = False, figsize = (12,12))\n",
    "\n",
    "#precision_recall_curve\n",
    "plot_precision_recall_curve(xgb_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.1. XGBoost Classifier - Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T20:49:30.477712Z",
     "start_time": "2020-10-11T20:48:52.471801Z"
    }
   },
   "outputs": [],
   "source": [
    "#cross validation scores\n",
    "xgb_cv = cross_val_score(xgb_model,X_train, y_train.ravel(), cv = 10, n_jobs = -1)\n",
    "\n",
    "#mean and standard deviation\n",
    "print(\"Average: {} +/- Std {}\".format(np.round(np.mean(xgb_cv),4),np.round(np.std(xgb_cv),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5. K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:54:53.775103Z",
     "start_time": "2020-10-15T00:54:44.655441Z"
    }
   },
   "outputs": [],
   "source": [
    "#model definition\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "#fit model\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "#predict model\n",
    "yhat_knn = knn.predict(X_test)\n",
    "\n",
    "# AUC-ROC\n",
    "knn_cv = cross_val_predict(knn, X_train, y_train, cv=5)\n",
    "knn_roc = roc_auc_score(y_train, knn_cv)\n",
    "\n",
    "\n",
    "#performance\n",
    "result_knn = pd.DataFrame([\"KNearestNeighbors\",accuracy_score(y_test,yhat_knn),cohen_kappa_score(y_test,yhat_knn),\n",
    "                          recall_score(y_test,yhat_knn), f1_score(y_test,yhat_knn),knn_roc,\n",
    "                          precision_score(y_test,yhat_knn)]).T\n",
    "result_knn.columns = [\"Model\",'Accuracy',\"Kappa Score\",\"Recall\",\"F1-Score\",\"roc_auc_score\",\"Precision_score\"]\n",
    "\n",
    "#classification_report\n",
    "print(classification_report(y_test, yhat_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:54:57.484013Z",
     "start_time": "2020-10-15T00:54:53.778530Z"
    }
   },
   "outputs": [],
   "source": [
    "#confusion_matrix\n",
    "mt.plot_confusion_matrix(y_test,yhat_knn, normalize = False, figsize = (12,12))\n",
    "\n",
    "#precision_recall_curve\n",
    "plot_precision_recall_curve(knn, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.1. KNN - Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T20:52:40.290710Z",
     "start_time": "2020-10-11T20:52:31.367483Z"
    }
   },
   "outputs": [],
   "source": [
    "#cross validation scores\n",
    "knn_cv = cross_val_score(knn,X_train, y_train.ravel(), cv = 10, n_jobs = -1)\n",
    "\n",
    "#mean and standard deviation\n",
    "print(\"Average: {} +/- Std {}\".format(np.round(np.mean(knn_cv),4),np.round(np.std(knn_cv),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6. LGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:56:08.984288Z",
     "start_time": "2020-10-15T00:56:05.564445Z"
    }
   },
   "outputs": [],
   "source": [
    "#define model\n",
    "model_lgbm = LGBMClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "#training model\n",
    "model_lgbm.fit(X_train, y_train)\n",
    "\n",
    "#predict model\n",
    "yhat_lgbm = model_lgbm.predict(X_test)\n",
    "\n",
    "# AUC-ROC\n",
    "lgbm_cv = cross_val_predict(model_lgbm, X_train, y_train, cv=5)\n",
    "lgbm_roc = roc_auc_score(y_train, lgbm_cv)\n",
    "\n",
    "\n",
    "#performance\n",
    "result_lgbm = pd.DataFrame([\"LGBM Classifier\",accuracy_score(y_test,yhat_lgbm),cohen_kappa_score(y_test,yhat_lgbm),\n",
    "                          recall_score(y_test,yhat_lgbm), f1_score(y_test,yhat_lgbm),lgbm_roc,\n",
    "                           precision_score(y_test,yhat_lgbm)]).T\n",
    "result_lgbm.columns = [\"Model\",'Accuracy',\"Kappa Score\",\"Recall\",\"F1-Score\",\"roc_auc_score\",\"Precision_score\"]\n",
    "\n",
    "#classification_report\n",
    "print(classification_report(y_test, yhat_lgbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:24:47.289798Z",
     "start_time": "2020-10-15T00:24:47.277183Z"
    }
   },
   "outputs": [],
   "source": [
    "#result lgbg\n",
    "result_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:24:49.936371Z",
     "start_time": "2020-10-15T00:24:48.969798Z"
    }
   },
   "outputs": [],
   "source": [
    "#confusion_matrix\n",
    "mt.plot_confusion_matrix(y_test,yhat_lgbm, normalize = False, figsize = (12,12))\n",
    "\n",
    "#precision_recall_curve\n",
    "plot_precision_recall_curve(model_lgbm, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6.1. LGBM - Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:25:02.755106Z",
     "start_time": "2020-10-15T00:24:52.536668Z"
    }
   },
   "outputs": [],
   "source": [
    "#cross validation scores\n",
    "lgbm_cv = cross_val_score(model_lgbm,X_train, y_train.ravel(), cv = 10, n_jobs = -1)\n",
    "\n",
    "#mean and standard deviation\n",
    "print(\"Average: {} +/- Std {}\".format(np.round(np.mean(lgbm_cv),4),np.round(np.std(lgbm_cv),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:55:14.781540Z",
     "start_time": "2020-10-15T00:54:57.489204Z"
    }
   },
   "outputs": [],
   "source": [
    "#define model\n",
    "rf = RandomForestClassifier(n_jobs = -1)\n",
    "\n",
    "#training model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "#predict model\n",
    "yhat_rf = rf.predict(X_test)\n",
    "\n",
    "# AUC-ROC\n",
    "rf_cv = cross_val_predict(rf, X_train, y_train, cv=5)\n",
    "rf_roc = roc_auc_score(y_train, rf_cv)\n",
    "\n",
    "#performance\n",
    "result_rf = pd.DataFrame([\"Random Forest Classifier\",accuracy_score(y_test,yhat_rf),cohen_kappa_score(y_test,yhat_rf),\n",
    "                          recall_score(y_test,yhat_rf), f1_score(y_test,yhat_rf),rf_roc,\n",
    "                           precision_score(y_test,yhat_rf)]).T\n",
    "result_rf.columns = [\"Model\",'Accuracy',\"Kappa Score\",\"Recall\",\"F1-Score\",\"roc_auc_score\",\"Precision_score\"]\n",
    "\n",
    "#classification_report\n",
    "print(classification_report(y_test, yhat_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:55:16.528152Z",
     "start_time": "2020-10-15T00:55:14.787804Z"
    }
   },
   "outputs": [],
   "source": [
    "#confusion_matrix\n",
    "mt.plot_confusion_matrix(y_test,yhat_rf, normalize = False, figsize = (12,12))\n",
    "\n",
    "#precision_recall_curve\n",
    "plot_precision_recall_curve(rf, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7.1. RandomForestClassifier - Cross-Validation - Real Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T20:54:50.081644Z",
     "start_time": "2020-10-11T20:54:00.803924Z"
    }
   },
   "outputs": [],
   "source": [
    "#cross validation scores\n",
    "rf_cv = cross_val_score(rf,X_train, y_train.ravel(), cv = 10, n_jobs = -1)\n",
    "\n",
    "#mean and standard deviation\n",
    "print(\"Average: {} +/- Std {}\".format(np.round(np.mean(rf_cv),4),np.round(np.std(rf_cv),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.8. Performance Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.8.1. Summary Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T13:43:51.209227Z",
     "start_time": "2020-10-11T13:43:51.103977Z"
    }
   },
   "outputs": [],
   "source": [
    "#concat all the models as a dataframe\n",
    "result_model = pd.concat([result_log,result_svm,result_xgb,result_knn,result_lgbm,result_rf])\n",
    "result_model.sort_values(\"Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T21:02:14.687944Z",
     "start_time": "2020-10-11T21:02:14.677380Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = pd.DataFrame([np.mean(log_cv),np.mean(svm_cv),np.mean(xgb_cv),np.mean(knn_cv),np.mean(lgbm_cv),np.mean(rf_cv)])\n",
    "std = pd.DataFrame([np.std(log_cv),np.std(svm_cv),np.std(xgb_cv),np.std(knn_cv),np.std(lgbm_cv),np.std(rf_cv)])\n",
    "models = pd.DataFrame(['Logistc Regression', \"Support Vector Machine\",\"XGBoost Classifier\", \"KNearest Neighbors\",\"LGBM Classifier\",\"Random Forest Classifier\"])\n",
    "colunas = ['Modelo']\n",
    "\n",
    "models.columns = colunas\n",
    "models['Accuracy'] = accuracy\n",
    "models['Standard Deviation'] = std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T21:07:36.881233Z",
     "start_time": "2020-10-11T21:07:36.862572Z"
    }
   },
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.8.2. Roc-curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:56:16.890092Z",
     "start_time": "2020-10-15T00:56:14.795751Z"
    }
   },
   "outputs": [],
   "source": [
    "#machine leraning model\n",
    "lr_probs = logreg.predict_proba(X_test)\n",
    "lgbm_probs = model_lgbm.predict_proba(X_test)\n",
    "xgb_probs = xgb_model.predict_proba(X_test)\n",
    "knn_probs = knn.predict_proba(X_test)\n",
    "rf_probs = rf.predict_proba(X_test)\n",
    "\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "lr_probs = lr_probs[:, 1]\n",
    "lgbm_probs = lgbm_probs[:,1]\n",
    "xgb_probs = xgb_probs[:,1]\n",
    "knn_probs = knn_probs[:,1]\n",
    "rf_probs = rf_probs[:,1]\n",
    "\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "lgbm_auc = roc_auc_score(y_test, lgbm_probs)\n",
    "xgb_auc = roc_auc_score(y_test, xgb_probs)\n",
    "knn_auc = roc_auc_score(y_test, knn_probs)\n",
    "rf_auc = roc_auc_score(y_test, rf_probs)\n",
    "\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
    "print('LGBM: ROC AUC=%.3f' % (lgbm_auc))\n",
    "print('XGBoostClassifier: ROC AUC=%.3f' % (xgb_auc))\n",
    "print('KNearestNeighbors: ROC AUC=%.3f' % (knn_auc))\n",
    "print('RandomForestClassifier: ROC AUC=%.3f' % (rf_auc))\n",
    "\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
    "lgbm_fpr, lgbm_tpr, _ = roc_curve(y_test, lgbm_probs)\n",
    "xgb_fpr, xgb_tpr, _ = roc_curve(y_test, xgb_probs)\n",
    "knn_fpr, knn_tpr, _ = roc_curve(y_test, knn_probs)\n",
    "rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_probs)\n",
    "\n",
    "#plot\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "plt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "plt.plot(lgbm_fpr, lgbm_tpr, marker='.', label='LGBM')\n",
    "plt.plot(xgb_fpr, xgb_tpr, marker='.', label='XGBoostClassifier')\n",
    "plt.plot(knn_fpr, knn_tpr, marker='.', label='KNN')\n",
    "plt.plot(rf_fpr, rf_tpr, marker='.', label='RandomForestClassifier')\n",
    "\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.0. HYPERPARAMETER FINE TUNNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned. \n",
    "\n",
    "Therefore, we will use the ** RandomSearchCV ** algorithm beacuse, the method described herein is a type of local random search, where every iteration is dependent on the prior iteration's candidate solution. So, we will spend less time to obtain the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. Random Search CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the model we choose, we will apply the RandomSearchCv technique to obtain the best parameters from our algorithm. This will allow us to increase the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:28:35.301985Z",
     "start_time": "2020-10-15T00:25:03.932123Z"
    }
   },
   "outputs": [],
   "source": [
    "#define model\n",
    "model_lgbm = LGBMClassifier(n_jobs = -1, random_state = 42)\n",
    "\n",
    "#parameters\n",
    "parameters = {'max_depth': np.arange(2, 12, 2), \n",
    "         'num_leaves': 2 ** np.arange(2, 10, 2),\n",
    "         'min_data_in_leaf': np.arange(100, 1050, 50), \n",
    "         'learning_rate': np.linspace(0.001, 0.6, 15),\n",
    "         'colsample_bytree': np.linspace(0.1, 1, 5),\n",
    "         'subsample': np.linspace(0.25, 1, 15),\n",
    "         'n_estimators': np.arange(10, 105, 15)}\n",
    "\n",
    "#define RandomSearchCV\n",
    "lgbm_random_cv = RandomizedSearchCV(estimator=model_lgbm, param_distributions=parameters,\n",
    "                                scoring='accuracy', n_iter=100, cv=10, verbose=2,\n",
    "                                random_state=42, n_jobs=-1)\n",
    "\n",
    "#fit RandomSearchCV\n",
    "lgbm_random_cv.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:50:56.415924Z",
     "start_time": "2020-10-15T00:50:56.409498Z"
    }
   },
   "outputs": [],
   "source": [
    "#get the best parameters to our model\n",
    "lgbm_random_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. Fit the model using the best parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:56:31.704162Z",
     "start_time": "2020-10-15T00:56:31.022090Z"
    }
   },
   "outputs": [],
   "source": [
    "#define model\n",
    "lgbm_hyper = LGBMClassifier(learning_rate=0.17214285714285713, max_depth=8,\n",
    "               min_data_in_leaf=950, n_estimators=85, num_leaves=256,\n",
    "               random_state=42, subsample=0.5714285714285714)\n",
    "\n",
    "#fit model\n",
    "lgbm_hyper.fit(X_train, y_train)\n",
    "\n",
    "#predict model\n",
    "yhat_lgbm_hyper = lgbm_hyper.predict(X_test)\n",
    "\n",
    "#Performance\n",
    "#performance\n",
    "result_lgbm_hyper = pd.DataFrame([\"LGBM Classifier\",lgbm_random_cv.best_score_,accuracy_score(y_test,yhat_lgbm_hyper),cohen_kappa_score(y_test,yhat_lgbm_hyper),\n",
    "                          recall_score(y_test,yhat_lgbm_hyper), f1_score(y_test,yhat_lgbm_hyper),lgbm_auc,\n",
    "                           precision_score(y_test,yhat_lgbm_hyper)]).T\n",
    "result_lgbm_hyper.columns = [\"Model\",'Best Score','Accuracy',\"Kappa Score\",\"Recall\",\"F1-Score\",\"ROC_AUC\",\"Precision_score\"]\n",
    "\n",
    "#classification_report\n",
    "print(classification_report(y_test, yhat_lgbm_hyper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:58:50.206854Z",
     "start_time": "2020-10-15T00:58:50.187763Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(lgbm_hyper,open(\"./parameters/model_cardio_lgbm.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T00:56:36.239065Z",
     "start_time": "2020-10-15T00:56:36.223610Z"
    }
   },
   "outputs": [],
   "source": [
    "result_lgbm_hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T21:24:38.101070Z",
     "start_time": "2020-10-11T21:24:37.018458Z"
    }
   },
   "outputs": [],
   "source": [
    "#confusion_matrix\n",
    "mt.plot_confusion_matrix(y_test,yhat_lgbm_hyper, normalize = False, figsize = (15,10))\n",
    "\n",
    "#precision_recall_curve\n",
    "plot_precision_recall_curve(lgbm_hyper, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4. Real Performance - Cross-Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply the cross-validation technique to obtain the real performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T21:27:29.680859Z",
     "start_time": "2020-10-11T21:27:24.959910Z"
    }
   },
   "outputs": [],
   "source": [
    "#cross validation scores\n",
    "lgbm_cv = cross_val_score(lgbm_hyper,X_train, y_train.ravel(), cv = 10, n_jobs = -1)\n",
    "\n",
    "#mean and standard deviation\n",
    "print(\"Average: {} +/- Std {}\".format(np.round(np.mean(lgbm_cv),4),np.round(np.std(lgbm_cv),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.0. BUSINESS PERFORMANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remember what is our business problem:\n",
    "- **Each diagnosis costs about $1000,00**\n",
    "\n",
    "- **The price will vary according to the precision, the customer pays $500.00 for every 5% precision above 50%.**\n",
    "\n",
    "- **If the diagnostic precision is 50% or below, the customer doesn't pay for it.**\n",
    "\n",
    "What did we get?\n",
    "\n",
    "- **Precision = 76.04%**\n",
    "- **5% above 50% = $500,00**\n",
    "\n",
    "- **25% above 50% = $2500,00**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1. Accuracy and Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T21:27:59.148586Z",
     "start_time": "2020-10-11T21:27:59.130165Z"
    }
   },
   "outputs": [],
   "source": [
    "#calculate Accuracy and Precision\n",
    "result_lgbm_hyper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2. Confidence Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T21:28:10.960988Z",
     "start_time": "2020-10-11T21:28:10.937658Z"
    }
   },
   "outputs": [],
   "source": [
    "# binomial confidence interval\n",
    "interval = 1.96 * sqrt( (0.2385 * (1 - 0.2385)) / X_test.shape[0])\n",
    "print('%.3f' % interval)\n",
    "\n",
    "conf_interval = ['Confidence Interval', \"Worst Scenario\", \"Best Scenario\"]\n",
    "results = [interval, 0.2385 + interval, 0.2385 - interval]\n",
    "\n",
    "conf = pd.DataFrame(results).T\n",
    "conf.columns = conf_interval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3. Financial Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T01:12:01.626582Z",
     "start_time": "2020-10-15T01:12:01.609156Z"
    }
   },
   "outputs": [],
   "source": [
    "#calculate financial feedback\n",
    "business_perf = ['LGBM Classifier', \"$ {:,.2f}\".format(500*70000), \"$ {:,.2f}\".format(2500*len(df_raw)),\"$ 140.000.000,00\", \"500%\"]\n",
    "df_business = pd.DataFrame(business_perf).T\n",
    "df_business.columns = [\"Model\",\"Worst Scenario\",\"Best Scenario\",\"Gain\",\"% Gain\"]\n",
    "df_business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a data set of 70000 people and each exam costing approximately $ 2500.00, our financial return will be, in the worst case, $ 35,000,000.00 and in the best case, $ 175,000,000.00."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
